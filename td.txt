- 
Amazon Redshift can automatically sort and perform a VACUUM DELETE operation on tables in the background. To clean up tables after a load or a series of incremental updates, you can also run the VACUUM command, either against the entire database or against individual tables
Only the table owner or a superuser can effectively vacuum a table.
The VACUUM process re-sorts rows and reclaims space in either a specified table or all tables in the current database in Amazon Redshift
By default, VACUUM skips the sort phase for any table where more than 95 percent of the table’s rows are already sorted. Skipping the sort phase can significantly improve VACUUM performance. It can slow down because of the following reasons:
– A high percentage of unsorted data
– Large table with too many columns
– Interleaved sort key usage
– Irregular or infrequent use of VACUUM
– Concurrent tables, cluster queries, DDL statements, or ETL jobs
Use the svv_vacuum_progress query to check the status and details of your VACUUM operation. It is recommended to use the VACUUM command with the BOOST option. The BOOST option allocates additional resources to VACUUM, such as available memory and disk space.
With the BOOST option, VACUUM operates in one window and blocks concurrent deletes and updates for the duration of the VACUUM operation.

-
an automated snapshot in Redshift only takes a snapshot every eight hours.


-
The UNLOAD command unloads the result of an Amazon Redshift query to your Amazon S3 data lake in Apache Parquet, an efficient open columnar storage format for analytics. It allows to save money, freeing up memory in Redshift.
It supports Redshift, it doesn't support Redshift spectrum.

-
COPY fails to load data to Amazon Redshift if the CSV file uses carriage returns (“\\r”, “^M”, or “0x0D” in hexadecimal) as a line terminator. 
When the COPY command has the IGNOREHEADER parameter set to a non-zero number, Amazon Redshift skips the first line, and therefore, the entire file. No load errors are returned because the operation is technically successful

-
To resize your cluster (can be done on a schedule), use one of the following approaches in Redshift:
	• Elastic resize (fastest method) – To quickly add or remove nodes from an existing cluster, use elastic resize. You can use it to change the node type, number of nodes, or both. If you only change the number of nodes then queries are temporarily paused and connections are held open if possible. During the resize operation, the cluster is read-only. Typically, elastic resize takes 10–15 minutes.
	• Classic resize (takes several hours to complete) – Use classic resize to change the node type, number of nodes, or both. Choose this option when you are resizing to a configuration that isn’t available through elastic resize. A classic resize copies tables to a new cluster. The source cluster will be in read-only mode until the resize operation finishes. An example is to or from a single-node cluster. During the resize operation, the cluster is read-only. Typically, classic resize takes 2 hours–2 days or longer, depending on your data’s size.
	• Snapshot and restore with classic resize – To keep your cluster available during a classic resize, you can first make a copy of an existing cluster, then resize the new cluster. Keep in mind that all data written to the source cluster after the snapshot is taken must be manually copied to the target cluster after the migration.


-
In AWS Glue you can’t create a data catalog in Amazon RDS. This is only applicable in AWS Glue

-
EMR
you cannot use Amazon EMR as a data source for Amazon QuickSight

-
AWS offers out-of-the-box Amazon Elastic MapReduce (Amazon EMR) integration with Amazon DynamoDB, providing customers an integrated solution that eliminates the often prohibitive costs of administration, maintenance, and upfront hardware

-
S3 select and Glacier Select
In contrast, cold data stored in Glacier can now be easily queried within minutes and glacier select only accepts uncompressed CSV (S3 select accepts GZIP or BZIP2 for CSV or JSON).
S3 selects costs less than Athena.
Redshift spectrum too can query GZIP-compressed CSV files but it costs more than S3 select.


-
Amazon Athena allows you to set two types of cost controls:
– per-query limit (amount of data scanned per query)
– per-workgroup limit (a.k.a workgroup-wide data usage control limit)
You can associate cloudwatch alarms to trigger SNS notifications.

With these controls within Athena it is unnecessary to write lambda functions.
-
By partitioning your data in Athena, you can restrict the amount of data scanned by each query, thus improving performance and reducing cost. 
Since data gets loaded once a day, it is more suitable to partition it based on device and date instead of by year, month, day and hour;

-
Athena works with S3 but not with Glacier
-
Athena can query buckets not publicly available as well as inter-region queries
-
Customers can connect to Amazon Athena using ODBC and JDBC drivers

-
Amazon Kinesis Data Firehose can convert the format of your input data from JSON to Apache Parquet or Apache ORC before storing the data in Amazon S3. Parquet and ORC are columnar data formats that save space and enable faster queries than row-oriented formats like JSON
